{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Restaurant Photos Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yelp held a photo classification competition on Kaggle two years ago. It asked Kagglers\n",
    "to build a model that automatically tags user-uploaded photos with multiple labels, nine\n",
    "labels to be exact. In this capstone project, I would be working on designing and\n",
    "building a Convolutional Neural Network to try to achieve or better the highest\n",
    "benchmark score. The goal of this project is to assign a set of labels to each photo\n",
    "correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dropout, Dense, GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from sklearn.datasets import load_files       \n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import ImageFile\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Datasets\n",
    "\n",
    "**train_photo_to_biz_ids.csv.tgz** contain the mapping\n",
    "from each photo to its associated business ID. In other words, this mapping allows a\n",
    "business to have more than one pictures, and we're able to tell which photos belong to\n",
    "which store.\n",
    "\n",
    "**train_photos.tgz** contain the actual photos. Photo ID is in each\n",
    "photo's file name. All the images are user-uploaded, meaning they are non-uniform in\n",
    "sizes and color images.\n",
    "\n",
    "**train.csv.tgz** contains each business ID with its associated correct/truth labels. There are\n",
    "2000 distinct businesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████▉                                                                                                         | 27432/234842 [01:36<13:02, 265.05it/s]"
     ]
    }
   ],
   "source": [
    "# load train and validation datasets\n",
    "def load_dataset(path, photos_to_biz, labels, should_split):\n",
    "    # get filenames\n",
    "    processed_filenames, processed_labels = [], []\n",
    "    filenames = os.listdir(path=path)\n",
    "    for filename in filenames:\n",
    "        # remove filename that contains underscore\n",
    "        if '_' not in filename:\n",
    "            processed_filenames.append(filename)\n",
    "    \n",
    "    # delete this line when actually training!!!!!!!!!!!!!!!! only test code works for now\n",
    "#     processed_filenames = processed_filenames[:8000]\n",
    "    \n",
    "    # get each photo's target labels\n",
    "    for filename in tqdm(processed_filenames):\n",
    "        bus_id = photos_to_biz.query('train_photo_id=={}'.format(filename.split(\".\", 1)[0]))['business_id']\n",
    "        this_labels = labels.query(\"business_id=={}\".format(bus_id.iloc[0]))['labels'].iloc[0]\n",
    "        \n",
    "        tmp_labels = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        if isinstance(this_labels, float) and math.isnan(this_labels):\n",
    "            # empty cell\n",
    "            pass\n",
    "        else:\n",
    "            # multiple labels\n",
    "            this_labels = this_labels.split(\" \")\n",
    "            for label in this_labels:\n",
    "                tmp_labels[int(label)] = 1\n",
    "\n",
    "        processed_labels.append(tmp_labels)\n",
    "    \n",
    "    if should_split:\n",
    "        X_train, X_validation, y_train, y_validation = train_test_split(np.array(processed_filenames), \n",
    "                                                                        np.array(processed_labels), \n",
    "                                                                        test_size = 0.2, \n",
    "                                                                        random_state = 0)\n",
    "\n",
    "        return X_train, X_validation, y_train, y_validation\n",
    "    \n",
    "    # just return filenames and labels\n",
    "    return np.array(processed_filenames), np.array(processed_labels)\n",
    "\n",
    "# img_folder = '../'\n",
    "img_folder = \"C:\\\\Users\\\\I-Chun Liu\\\\Documents\\\\Local_Code\\\\final_project\\\\data\"\n",
    "\n",
    "# test_photos_to_biz = pd.read_csv(img_folder + '/test_photo_to_biz.csv.tgz', compression='gzip', sep=',')\n",
    "# test_photos_to_biz.columns = ['test_photo_id', 'business_id']\n",
    "\n",
    "train_photo_to_biz_ids = pd.read_csv(img_folder + '/train_photo_to_biz_ids.csv.tgz', compression='gzip', sep=',')\n",
    "train_photo_to_biz_ids.columns = ['train_photo_id', 'business_id']\n",
    "\n",
    "train_labels = pd.read_csv(img_folder + '/train.csv.tgz', compression='gzip', sep=',')\n",
    "train_labels.columns = ['business_id', 'labels']\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = load_dataset(img_folder + '/train_photos', train_photo_to_biz_ids, train_labels, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Validation set has {} samples.\".format(X_validation.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_filenames):\n",
    "    list_of_tensors = [path_to_tensor(img_folder + '/train_photos/' + img_filename) for img_filename in tqdm(img_filenames)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensors = paths_to_tensor(X_train).astype('float32')/255\n",
    "print(\"Train tenors size: {}\".format(train_tensors.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, subplots_array_before = plt.subplots(4,4, figsize=(20, 20))\n",
    "f1.suptitle('Before Image Augmentations:', fontsize=35)\n",
    "for i in range(0, 16):\n",
    "    row, col = 0, 0\n",
    "    for i in range(0, 16):\n",
    "        if col == 4:\n",
    "            col = 0\n",
    "            row += 1\n",
    "\n",
    "        subplots_array_before[row, col].imshow(train_tensors[i], interpolation='nearest', aspect='auto')\n",
    "        col += 1\n",
    "\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform image augmentations\n",
    "datagen_train = image.ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.05,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest')\n",
    "\n",
    "datagen_train.fit(train_tensors)\n",
    "\n",
    "f2, subplots_array_after = plt.subplots(4,4, figsize=(20, 20))\n",
    "f2.suptitle('After Image Augmentations:', fontsize=35)\n",
    "for X_batch in datagen_train.flow(train_tensors):\n",
    "    row, col = 0, 0\n",
    "    for i in range(0, 16):\n",
    "        if col == 4:\n",
    "            col = 0\n",
    "            row += 1\n",
    "\n",
    "        subplots_array_after[row, col].imshow(X_batch[i], interpolation='nearest', aspect='auto')\n",
    "        col += 1\n",
    "\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "train_features = extract_Resnet50(train_tensors * 255)\n",
    "\n",
    "print(\"Train features shape: {}\".format(train_features.shape))\n",
    "\n",
    "np.save('./bottleneck_features/Resnet50_train', train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process / Feature Extraction of Validation Data\n",
    "\n",
    "Do the same image pre-processing and bottleneck features extraction for validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1600/1600 [00:02<00:00, 628.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation tenors size: (1600, 224, 224, 3)\n",
      "processing tensor...\n",
      "extracting features using Resnet50...\n",
      "Validation features shape: (1600, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "validation_tensors = paths_to_tensor(X_validation).astype('float32')/255\n",
    "print(\"Validation tenors size: {}\".format(validation_tensors.shape))\n",
    "\n",
    "# image pre-processing\n",
    "datagen_validation = image.ImageDataGenerator(\n",
    "                    rotation_range=20,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.2,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "datagen_validation.fit(validation_tensors)\n",
    "\n",
    "validation_features = extract_Resnet50(validation_tensors * 255)\n",
    "\n",
    "print(\"Validation features shape: {}\".format(validation_features.shape))\n",
    "\n",
    "np.save('./bottleneck_features/Resnet50_validation', validation_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create / Train the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (6400, 7, 7, 2048)\n",
      "Validation features shape: (1600, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Load bottleneck features\n",
    "train_features = np.load('./bottleneck_features/Resnet50_train.npy')\n",
    "validation_features = np.load('./bottleneck_features/Resnet50_validation.npy')\n",
    "\n",
    "print(\"Train features shape: {}\".format(train_features.shape))\n",
    "print(\"Validation features shape: {}\".format(validation_features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 10.9633 - acc: 0.1289 - val_loss: 10.2990 - val_acc: 0.0919\n",
      "Epoch 2/20\n",
      " - 3s - loss: 10.4404 - acc: 0.1412 - val_loss: 10.4138 - val_acc: 0.1113\n",
      "Epoch 3/20\n",
      " - 3s - loss: 10.2893 - acc: 0.1542 - val_loss: 10.2021 - val_acc: 0.1844\n",
      "Epoch 4/20\n",
      " - 3s - loss: 10.1655 - acc: 0.1603 - val_loss: 9.9945 - val_acc: 0.1294\n",
      "Epoch 5/20\n",
      " - 3s - loss: 10.1140 - acc: 0.1475 - val_loss: 10.1416 - val_acc: 0.2875\n",
      "Epoch 6/20\n",
      " - 3s - loss: 10.0556 - acc: 0.1559 - val_loss: 10.1759 - val_acc: 0.0494\n",
      "Epoch 7/20\n",
      " - 3s - loss: 10.0248 - acc: 0.1541 - val_loss: 10.2804 - val_acc: 0.1069\n",
      "Epoch 8/20\n",
      " - 3s - loss: 9.9883 - acc: 0.1577 - val_loss: 10.6207 - val_acc: 0.0675\n",
      "Epoch 9/20\n",
      " - 3s - loss: 9.9535 - acc: 0.1580 - val_loss: 10.3900 - val_acc: 0.1106\n",
      "Epoch 10/20\n",
      " - 3s - loss: 9.9354 - acc: 0.1630 - val_loss: 10.2158 - val_acc: 0.1594\n",
      "Epoch 11/20\n",
      " - 3s - loss: 9.9049 - acc: 0.1669 - val_loss: 10.2989 - val_acc: 0.1075\n",
      "Epoch 12/20\n",
      " - 3s - loss: 9.9000 - acc: 0.1736 - val_loss: 10.3031 - val_acc: 0.2231\n",
      "Epoch 13/20\n",
      " - 3s - loss: 9.8635 - acc: 0.1689 - val_loss: 10.1904 - val_acc: 0.0956\n",
      "Epoch 14/20\n",
      " - 3s - loss: 9.8615 - acc: 0.1695 - val_loss: 10.3696 - val_acc: 0.0612\n",
      "Epoch 15/20\n",
      " - 3s - loss: 9.8393 - acc: 0.1669 - val_loss: 10.4076 - val_acc: 0.0887\n",
      "Epoch 16/20\n",
      " - 3s - loss: 9.8268 - acc: 0.1772 - val_loss: 10.2439 - val_acc: 0.1156\n",
      "Epoch 17/20\n",
      " - 3s - loss: 9.8117 - acc: 0.1741 - val_loss: 10.2510 - val_acc: 0.1656\n",
      "Epoch 18/20\n",
      " - 3s - loss: 9.8042 - acc: 0.1736 - val_loss: 10.2983 - val_acc: 0.2469\n",
      "Epoch 19/20\n",
      " - 3s - loss: 9.8012 - acc: 0.1873 - val_loss: 10.1309 - val_acc: 0.2756\n",
      "Epoch 20/20\n",
      " - 3s - loss: 9.7828 - acc: 0.1778 - val_loss: 10.3798 - val_acc: 0.4375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22a92519940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(GlobalAveragePooling2D(input_shape=train_features.shape[1:]))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_features, y_train, epochs=epochs, batch_size=batch_size, validation_data=(validation_features, y_validation), verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05492581, 0.11509427, 0.11095002, 0.17964624, 0.02704421,\n",
       "       0.08612281, 0.20188424, 0.02082847, 0.20350394], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(validation_features)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. How to use pretrained model in Keras: https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07413185, 0.08587088, 0.02096588, 0.07380328, 0.01188321,\n",
       "       0.02914063, 0.17979518, 0.00446302, 0.5199461 ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(validation_features)[190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.379753036499023, 0.4375]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(validation_features, y_validation, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
